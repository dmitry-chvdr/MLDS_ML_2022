{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbd34bde-d941-41c4-aace-75ebc5264a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pymorphy2 in /home/dmitry/.local/lib/python3.8/site-packages (0.9.1)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /home/dmitry/.local/lib/python3.8/site-packages (from pymorphy2) (0.7.2)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /home/dmitry/.local/lib/python3.8/site-packages (from pymorphy2) (2.4.417127.4579844)\n",
      "Requirement already satisfied: docopt>=0.6 in /usr/lib/python3/dist-packages (from pymorphy2) (0.6.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/dmitry/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "!pip install pymorphy2\n",
    "import re\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from functools import lru_cache\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b13cda82-1a5d-4fc5-8bb6-0ba0d5386ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "303e9a95-58d3-4e7a-b625-f6a95c91e4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train_ml.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4922791e-77c9-4c0f-83fc-0f7468fa0bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удалим нулевые и отфармотируем дату"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33e56e0a-d80a-47b9-9487-dfb5a7550397",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'], format='%d.%m.%Y %H:%M')\n",
    "df = df.dropna()\n",
    "df = df.astype({'grades': 'int32'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b63582c3-49af-4c8f-9a9a-4bcc2d1cb590",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MorphAnalyzer()\n",
    "regex = re.compile(\"[А-Яа-я]+\")\n",
    "\n",
    "def words_only(text, regex=regex):\n",
    "    try:\n",
    "        return regex.findall(text.lower())\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "@lru_cache(maxsize=128)\n",
    "def lemmatize_word(token, pymorphy=m):\n",
    "    return pymorphy.parse(token)[0].normal_form\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatize_word(w) for w in text]\n",
    "\n",
    "\n",
    "mystopwords = stopwords.words('russian') \n",
    "def remove_stopwords(lemmas, stopwords = mystopwords):\n",
    "    return [w for w in lemmas if not w in stopwords and len(w) > 3]\n",
    "\n",
    "def clean_text(text):\n",
    "    tokens = words_only(text)\n",
    "    lemmas = lemmatize_text(tokens)\n",
    "    \n",
    "    return ' '.join(remove_stopwords(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080378fe-fcd7-4749-8a49-88fe88d09282",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(4) as p:\n",
    "    lemmas = list(tqdm(p.imap(clean_text, df['feeds']), total=len(df)))\n",
    "    \n",
    "df['lemmas'] = lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37a62783-3e54-4b03-b216-e4b9f40d8644",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df.grades\n",
    "y_train = y_train.reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f02dbd6-bb70-4b4f-9979-b53ceb01de4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.7 s, sys: 613 ms, total: 26.4 s\n",
      "Wall time: 26.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vec = TfidfVectorizer(ngram_range=(1, 2))\n",
    "bow = vec.fit_transform(lemmas)\n",
    "clf = LogisticRegression(C=100, random_state=42, max_iter=3000, warm_start=True, penalty=\"l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4b93ca-b1b7-4015-bb38-c4653002926f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# небольшой трюк: будем обучаться частями\n",
    "ran = np.arange(y_train.shape[0])\n",
    "inds = np.array_split(ran, 10) # разобъем на 10 частей\n",
    "# и подмешаем к каждому куску случайных объектов\n",
    "inds = [np.concatenate((chunk, np.array(random.sample(list(ran), k=1000))), axis=None) for chunk in inds]\n",
    "\n",
    "for chunk in tqdm(inds):\n",
    "    clf.fit(bow[chunk, :], y_train.iloc[chunk].values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff531a9c-9049-4c1a-b35b-ff44cf791482",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('new_test_ml.csv', index_col=0)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4435fcc-2a47-49d2-80ab-1a384d31d30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(4) as p:\n",
    "    lemmas_test = list(tqdm(p.imap(clean_text, test['feeds']), total=len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4e3c90-b668-4800-a000-7bd20d591aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(vec.transform(lemmas_test))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f4f06f-77e2-4a73-9f81-cc43b15fecf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = pd.DataFrame({'inds': test.index,\n",
    "                    'grades': pred})\n",
    "sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d744bc-d521-42ff-9587-f8b02c7f4bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sol.to_csv('new_baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814282f6-6262-400c-a559-0cbd7dcb99dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(pred, return_counts=True)\n",
    "np.asarray((unique, counts)).T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
